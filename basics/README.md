# 张量

> ***深度学习中用向量或矩阵运算提高计算效率*** 
> $\bm{w}^\top\bm{x} = w_1x_1+w_2x_2+\cdots+w_Nx_N$

$\bm{w} = \begin{bmatrix}
w_1w_2 \cdots w_N
\end{bmatrix}^\top$

$\bm{x} = \begin{bmatrix}
x_1x_2 \cdots x_N
\end{bmatrix}^\top$

<!-- $$
\bm{w}^\top = \begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_N
\end{bmatrix}
$$ -->

> 数据常用张量(Tensor)形式存储, 张量是矩阵的扩展与延伸, 认为是高阶的矩阵, 一阶张量是向量, 二阶张量是矩阵, 类似 Numpy 多维数组(ndarray), 可以具有任意多的维度

---
> **注意:** 这里的"维度"是"阶"的概念,和线性代数中向量的"维度"含义不同
---

> 张量大小用形状(shape)描述, 比如一个三维张量的形状是[2,2,5], 表示每一维(也称为轴(axis))的元素的数量, 即第0轴上的数量是2, 第1轴上的数量是2, 第2轴上的数量是5.

<div align="center">
  <img src="https://camo.githubusercontent.com/88c7f6cf2f1eb17d870d33e5cc394dc79a201f0a2e3a8133332efa32ea3aeb91/68747470733a2f2f61692d73747564696f2d7374617469632d6f6e6c696e652e63646e2e626365626f732e636f6d2f66636464353030343731623834326134383131626437616233663732346162346239323236666339346266343436383138393034653539636531626236653030" alt="三种维度的张量可视化表示" width="800">
</div>

> 张量中元素的类型可以是布尔型整数、整数、浮点数或者复数, 但同一张量中所有元素的数据类型均相同, 因此可以给张量一个定义一个数据类型(dtype)来表示其元素的类型

## 1 创建张量

### 1.1 指定数据创建张量

> 通过给定Python列表数据, 可以创建任意维度的张量

```python
ndim_1_tensor = torch.tensor([2.0, 3.0, 4.0])

ndim_2_tensor = torch.tensor([[2.0, 3.0, 4.0],
                                [1.0, 6.0, 7.0]])

ndim_3_tensor = torch.tensor([[[2.0, 3.0, 4.0],
                                [1.0, 6.0, 7.0]],
                                [[2.0, 3.0, 4.0],
                                [1.0, 6.0, 7.0]]])

# ---------------------------------------------------
# 在创建张量的同时也定义数据的类型
# torch.ones([2, 3]) 默认的数据类型是 torch.float32
# ---------------------------------------------------
ndim2_Tensor = torch.ones([2, 3], dtype=torch.int64)
```

> 需要注意的是,张量在任何一个维度上的元素数量必须相等.下面尝试定义一个在同一维度上元素数量不等的张量

### 1.2 指定形状创建

```python
zeros_tensor = torch.zeros([3, 2])

ones_tensor = torch.ones([3, 2])

full_tensor = torch.full([3, 2], 10)
```
### 1.3 指定区间创建

```python
arange_tensor  = torch.arange(start=1, end=5, step=1)

# res: tensor([1, 2, 3, 4])
```
## 2 张量的属性

### 2.1 张量的形状

> 张量具有如下形状属性

| 方法 | 含义 |
|:---|:---:|
| ***Tensor.ndim*** | 张量的维度 |
| ***Tensor.shape*** | 每个维度上元素的数量 |
| ***Tensor.shape[n]*** | 张量第n维的大小,第n维也称为轴(axis) |
| ***Tensor.size*** | 张量中全部元素的个数 |

<div align="center">
  <img src="https://camo.githubusercontent.com/35616717a23afffd5b23a2a2c7aba20430da47ade37b6b6de3ccd1b909e47642/68747470733a2f2f61692d73747564696f2d7374617469632d6f6e6c696e652e63646e2e626365626f732e636f6d2f64383436316566303939343534396139386331623235336632613331666536306564623762623230306239363462343361396332353831386632326133316336" alt="ndim/shape/axis/size四种属性间的区别" width="700">
</div>

```python
# ---------------------------------
# Input
# ---------------------------------
ones_tensor  = torch.ones([2, 3, 4, 5])
print(ones_tensor.ndim)
print(ones_tensor.shape)
print(ones_tensor.shape[-1])
print(ones_tensor.size())
print(ones_tensor.numel())
print(ones_tensor)

# ---------------------------------
# Output
# ---------------------------------
4
torch.Size([2, 3, 4, 5])
5
torch.Size([2, 3, 4, 5])
120
tensor([[[[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]],

         [[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]],

         [[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]]],


        [[[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]],

         [[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]],

         [[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]]]])
```

### 2.2 改变张量形状

```python
# ------------------------------
# Input
# ------------------------------
ndim_3_Tensor = torch.tensor([[[1, 2, 3, 4, 5],
                              [6, 7, 8, 9, 10]],

                              [[11, 12, 13, 14, 15],
                              [16, 17, 18, 19, 20]],
                              
                              [[21, 22, 23, 24, 25],
                              [26, 27, 28, 29, 30]]])

print(ndim_3_Tensor.shape)
print(ndim_3_Tensor)

reshape_Tensor = torch.reshape(ndim_3_Tensor, [2, 5, 3])
print(reshape_Tensor)

# -------------------------------
# Output
# -------------------------------
torch.Size([3, 2, 5])
tensor([[[ 1,  2,  3,  4,  5],
         [ 6,  7,  8,  9, 10]],

        [[11, 12, 13, 14, 15],
         [16, 17, 18, 19, 20]],

        [[21, 22, 23, 24, 25],
         [26, 27, 28, 29, 30]]])
tensor([[[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9],
         [10, 11, 12],
         [13, 14, 15]],

        [[16, 17, 18],
         [19, 20, 21],
         [22, 23, 24],
         [25, 26, 27],
         [28, 29, 30]]])
```
---
***笔记_reshape技巧:***

- -1 表示这个维度的值是从张量的元素总数和剩余维度推断出来的, 因此, 有且只有一个维度可以设置为 -1

- ~~0 表示实际的维数是从张量的对应维数中复制出来的, 因此shape中0所对应的索引值不能超过张量的总维度~~

---


```python
ndim_1_Tensor = ndim_3_Tensor.reshape([-1])

torch.Size([30])
tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])
```

---

> ***torch.unsqueeze 将张量中的一个或多个维度中插入尺寸为1的维度***

```python
ndim2_Tensor = torch.ones([5, 10])
print(ndim2_Tensor.shape)

new_ndim2_Tensor = torch.unsqueeze(ndim2_Tensor, axis=0)
print(new_ndim2_Tensor)
print(new_ndim2_Tensor.shape)

# -------------------------------------------
# Ouput
# -------------------------------------------
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
torch.Size([5, 10])
tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])
torch.Size([1, 5, 10])
```

### 2.3 张量的数据类型

```python
ndim2_Tensor = torch.ones([5, 10])
print(ndim2_Tensor.dtype)

# --------------------------------------------
# Output
# --------------------------------------------
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
torch.float32
```

> ***.dtype() 查看张量的数据类型***

> ***.to() 改变张量的数据类型***

```python
ndim2_Tensor = torch.ones([5, 10])
new_Tensor= ndim2_Tensor.to(torch.int64)

print(ndim2_Tensor.dtype)
print(new_Tensor.dtype)
print(new_Tensor)
# --------------------------------------------
# Output
# --------------------------------------------
torch.float32
torch.int64
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
```
### 2.4 张量的设备位置

> 初始化张量时可以通过place来指定分配的设备位置, 可支持的设备位置有三种: CPU、GPU和固定内存.

> 固定内存仅针对CPU张量, 用于加速后续CPU到GPU的数据传输

```python
ndim2_Tensor.device
# --------------------------------------------
# Output
# --------------------------------------------
cpu
```

## 3 张量与Numpy数组转换

```python
import torch
ndim2_Tensor = torch.ones([5, 10])

print(ndim2_Tensor.dtype)
print(ndim2_Tensor.device)

print(ndim2_Tensor)

ndim2_Numpy = ndim2_Tensor.numpy()

print(ndim2_Numpy)
print(ndim2_Numpy.dtype)
# --------------------------------------------
# Output
# --------------------------------------------
torch.float32
cpu
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]
float32
```

## 4 张量的访问

### 4.1 索引和切片

> 可以通过索引或切片方便地访问或修改张量

> 使用Python索引规则与Numpy索引规则, 具有以下特点

- 基于 $0 - (n-1)$ 的下标进行索引, 如果下标为负数, 则从尾部开始计算
- 通过冒号":"分隔切片参数start:end:step进行切片操作, 也就是访问start到end范围内的部分元素并生成一个新的序列. 其中start为切片的起始位置, end为切片的截止位置, step是切片的步长, 这三个参数均可缺省.

### 4.2 访问单维张量

> 针对一维张量, 对于单个轴进行索引和切片

```python
ndim_1_Tensor = torch.arange(start=1, end=10, step=1)

print(ndim_1_Tensor)
print(ndim_1_Tensor.dtype)

print(ndim_1_Tensor[0])
print(ndim_1_Tensor[1])
print(ndim_1_Tensor[-1])
print(ndim_1_Tensor[:3])
print(ndim_1_Tensor[::3])

# -------------------------------------------
# Output
# -------------------------------------------
tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
torch.int64
tensor(1)
tensor(2)
tensor(9)
tensor([1, 2, 3])
tensor([1, 4, 7])
```

### 4.3 访问高维张量

```python
ndim_2_Tensor = torch.tensor([[0, 1, 2, 3],
                              [4, 5, 6, 7],
                              [8, 9, 10, 11]])

print(ndim_2_Tensor)

print(ndim_2_Tensor[0])
print(ndim_2_Tensor[0, :])
print(ndim_2_Tensor[:, 0])
print(ndim_2_Tensor[:, -1])
print(ndim_2_Tensor[:])
print(ndim_2_Tensor[0, 1])

# ---------------------------------------------------
# Output
# ---------------------------------------------------
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
tensor([0, 1, 2, 3])
tensor([0, 1, 2, 3])
tensor([0, 4, 8])
tensor([ 3,  7, 11])
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
tensor(1)
```

### 4.4 修改张量

> 与访问张量类似, 可以在单个或多个轴上通过索引或切片操作来修改张量

---
**提醒:**

*慎重通过索引或切片操作来修改张量, 此操作仅会原地修改该张量的数值, 且原值不会被保存. 如果被修改的张量参与梯度计算, 将仅会使用修改后的数值, 这可能会给梯度计算引入风险*

---

```python
nim2_Tensor = torch.ones([2, 3], dtype=torch.float32)

print('Origin Tensor: ', nim2_Tensor)

# 修改第 1维为0
# 等价于 nim2_Tensor[0] = 0
# 等价于 nim2_Tensor[0:1] = 0
nim2_Tensor[0, :] = 0
print('Change Tensor: ', nim2_Tensor)

# 修改第 1维为2.1
nim2_Tensor[0:1] = 2.1
print('Change Tensor: ', nim2_Tensor)

# 修改全部Tensor
nim2_Tensor[...] = 3
print('Change Tensor: ', nim2_Tensor)

print(nim2_Tensor.dtype)

# --------------------------------------------
# Output
# --------------------------------------------
Origin Tensor:  tensor([[1., 1., 1.],
        [1., 1., 1.]])
Change Tensor:  tensor([[0., 0., 0.],
        [1., 1., 1.]])
Change Tensor:  tensor([[2.1000, 2.1000, 2.1000],
        [1.0000, 1.0000, 1.0000]])
Change Tensor:  tensor([[3., 3., 3.],
        [3., 3., 3.]])
torch.float32
```

## 5 张量的运算

### 5.1 张量加法
> 张量支持包含基础数学运算、 逻辑运算、 矩阵运算等100余种运算操作, 以加法为例, 有两种实现方式:

```python
x = torch.tensor([[1.1, 2.2], [3.3, 4.4]], dtype=torch.float64)
y = torch.tensor([[5.5, 6.6], [7.7, 8.8]], dtype=torch.float64)

print(x.dtype)

print(x)
print("Method 1_torch API: ", torch.add(x, y))
print("Method 2_张量类成员: ", x.add(y))

# ---------------------------------------------
# Output
# ---------------------------------------------
torch.float64
tensor([[1.1000, 2.2000],
        [3.3000, 4.4000]], dtype=torch.float64)
Method 1:  tensor([[ 6.6000,  8.8000],
        [11.0000, 13.2000]], dtype=torch.float64)
Method 2:  tensor([[ 6.6000,  8.8000],
        [11.0000, 13.2000]], dtype=torch.float64)
```
---
> ***笔记:*** 由于张量类成员函数操作更方便, 以下均从张量类成员函数的角度, 对常用张量操作介绍

---

### 5.2 数学运算

> 张量类的基础数学函数如下:

x.abs()           # 逐元素取绝对值
x.ceil()          # 逐元素向上取整
x.floor()         # 逐元素向下取整
x.round()         # 逐元素四舍五入

x.exp()           # 逐元素计算自然常数为底的指数
x.log()           # 逐元素计算x的自然对数
x.reciprocal()    # 逐元素求倒数
x.square()        # 逐元素计算平方
x.sqrt()          # 逐元素计算平方根

x.sin()           # 逐元素计算正弦
x.cos()           # 逐元素计算余弦

x.add(y)          # 逐元素加
x.subtract(y)     # 逐元素减
x.multiply(y)     # 逐元素乘（积）
x.divide(y)       # 逐元素除
x.mod(y)          # 逐元素除并取余
x.pow(y)          # 逐元素幂
x.max()           # 指定维度上元素最大值，默认为全部维度
x.min()           # 指定维度上元素最小值，默认为全部维度
x.prod()          # 指定维度上元素累乘，默认为全部维度
x.sum()           # 指定维度上元素的和，默认为全部维度

```python
# -------------------------------------------
# torch对 Pyhton数学运算相关的魔法函数进行了重写
# -------------------------------------------

x + y  -> x.add(y)            # 逐元素加
x - y  -> x.subtract(y)       # 逐元素减
x * y  -> x.multiply(y)       # 逐元素乘（积）
x / y  -> x.divide(y)         # 逐元素除
x % y  -> x.mod(y)            # 逐元素除并取余
x ** y -> x.pow(y)            # 逐元素幂
```

### 5.3 逻辑运算

> 张量类的逻辑运算函数如下:

```python
# -------------------------------------------
# torch对 Python逻辑比较相关的魔法函数也进行了重写
# -------------------------------------------

x.isfinite()        # 判断Tensor中元素是否是有限的数字，即不包括inf与nan
x.equal_all(y)      # 判断两个Tensor的全部元素是否相等，并返回形状为[1]的布尔类Tensor
x.equal(y)          # 判断两个Tensor的每个元素是否相等，并返回形状相同的布尔类Tensor
x.not_equal(y)      # 判断两个Tensor的每个元素是否不相等
x.less_than(y)      # 判断Tensor x的元素是否小于Tensor y的对应元素
x.less_equal(y)     # 判断Tensor x的元素是否小于或等于Tensor y的对应元素
x.greater_than(y)   # 判断Tensor x的元素是否大于Tensor y的对应元素
x.greater_equal(y)  # 判断Tensor x的元素是否大于或等于Tensor y的对应元素
x.allclose(y)       # 判断两个Tensor的全部元素是否接近
```
### 5.4 矩阵运算

```python
# -------------------------------------------------------
# 矩阵类还包含了矩阵运算相关的函数,如矩阵的转置、范数计算和乘法等
# 有些矩阵运算中也支持大于两维的张量,比如matmul函数，对最后两个维度进行矩阵乘.
# 比如x是形状为[j,k,n,m]的张量,另一个y是[j,k,m,p]的张量,
# 则x.matmul(y)输出的张量形状为[j,k,n,p]
# -------------------------------------------------------

x.t()                         # 矩阵转置
x.transpose([1, 0])           # 交换第 0 维与第 1 维的顺序
x.norm('fro')                 # 矩阵的弗罗贝尼乌斯范数
x.dist(y, p=2)                # 矩阵（x-y）的2范数
x.matmul(y)                   # 矩阵乘法
```

### 5.5 广播机制

> *允许在一些运算时使用不同形状的张量。通常来讲，如果有一个形状较小和一个形状较大的张量，会希望多次使用较小的张量来对较大的张量执行某些操作，看起来像是形状较小的张量首先被扩展到和较大的张量形状一致，然后再做运算*

> 广播机制的条件(参考Numpy广播机制:
1. 每个张量至少为一维张量
2. 从后往前比较张量的形状, 当前维度的大小要么相等, 要么其中一个等于1, 要么其中一个不存在
```python
# 当两个Tensor的形状一致时, 可以广播
x = torch.ones((2, 3, 4))
y = torch.ones((2, 3, 4))

z_1 = x + y
print(z_1)

z_2 = x.add(y)
print(z_2)

# ---------------------------------
# Output_相同形状的广播机制
# ---------------------------------
tensor([[[2., 2., 2., 2.],
         [2., 2., 2., 2.],
         [2., 2., 2., 2.]],

        [[2., 2., 2., 2.],
         [2., 2., 2., 2.],
         [2., 2., 2., 2.]]])
tensor([[[2., 2., 2., 2.],
         [2., 2., 2., 2.],
         [2., 2., 2., 2.]],

        [[2., 2., 2., 2.],
         [2., 2., 2., 2.],
         [2., 2., 2., 2.]]])
```

```python
x = torch.ones((2, 3, 1, 5))
y = torch.ones((3, 4, 1))

# 从后往前依次比较:
# 第一次: y的维度大小是 1, x的维度大小是5
# 第二次: x的维度大小是 1, y的维度大小是4
# 第三次: x 和 y的维度大小相同
# 第四次: y的维度不存在, x的维度存在, 且大小是2

z = x.add(y)
print(z)
print(z.shape)

# ------------------------------------------
# Output
# ------------------------------------------
tensor([[[[2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.]],

         [[2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.]],

         [[2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.]]],


        [[[2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.]],

         [[2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.]],

         [[2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.],
          [2., 2., 2., 2., 2.]]]])
torch.Size([2, 3, 4, 5])
```

```python
# -------------------------------------
#     torch.matmul 矩阵乘法的广播规则
#
# 1) 如果两个张量均为一维, 获得点积结果
# 2) 如果两个张量都是二维, 则获得矩阵与矩阵的乘积
# 3) 如果张量x是一维，y是二维，则将x的shape转换为[1, D]，与y进行矩阵相乘后再删除前置尺寸
# 4) 如果张量x是二维，y是一维，则获得矩阵与向量的乘积
# 5)  如果两个张量都是N维张量(N > 2),则根据广播规则广播非矩阵维度(除最后两个维度外其余维度).比如：如果输入x是形状为[j,1,n,m]的张量,另一个y是[k,m,p]的张量,则输出张量的形状为[j,k,n,p]
# 一维向量与二维矩阵相乘时，会根据维度匹配规则进行
# 广播
# 结果维度由不匹配的维度决定
# -------------------------------------
```
> **torch的API有原位操作(inplace)和非原位操作**
> *原位操作即在原张量上保存操作结果，非原位操作则不会修改原张量，而是返回一个新的张量来表示运算结果*
> *x.add(y)是非原位操作，x.add_(y)为原位操作*

# 算子
> 一个复杂的机器学习模型（比如神经网络）可以看作一个复合函数，输入是数据特征，输出是标签的值或概率, 简单起见, 假设一个由 $L$ 个函数复合的神经网络定义为:`

$$y=f_L(\cdots f_2(f_1(x)))$$

> 其中 $f_l(\cdot)$ 可以为带参数的函数, 也可以为不带参数的函数, $x$ 为输入特征, $y$ 为某种损失. 我们将从 $x$ 到 $y$ 的计算看作一个前向计算过程. 而神经网络的参数学习需要计算损失关于所有参数的偏导数(即梯度). 假设函数 $f_l(\cdot)$ 包含参数 $\theta_l$, 根据链式法则:

$$\begin{aligned}
\frac{\partial y}{\partial \theta_l} &= {\frac{\partial f_l}{\partial \theta_l}} \frac{\partial y}{\partial f_l} \\
&= \frac{\partial f_l}{\partial \theta_l} \frac{\partial f_{l+1}}{\partial f_l} \cdots \frac{\partial f_L}{\partial f_{L-1}} .
\end{aligned}$$

> 在实践中，一种比较高效的计算 $y$ 关于每个函数 $f_l$, 的偏导数的方式是利用递归进行反向计算.
**令:** $\delta_l\triangleq \frac{\partial y}{\partial f_l}$, 则有

$$\delta_{l-1} = \frac{\partial f_l}{\partial f_{l-1}} \delta_{l}$$

> 如果将函数 $f_l(\cdot)$ 称为前向函数, 则 $\delta_{l-1}$ 的计算称为函数 $f(x)$ 的反向函数

> 如果我们实现每个基础函数的前向函数和反向函数,就可以非常方便地通过这些基础函数组合出复杂函数，并通过链式法则反向计算复杂函数的偏导数.在深度学习框架中,这些基本函数的实现称为算子(Operator,Op).有了算子,就可以像搭积木一样构建复杂的模型

## 1 算子定义

> 算子是构建复杂机器学习模型的基础组件，包含一个函数的前向函数和反向函数。为了可以更便捷地进行算子组合，本书中定义算子 Op 的接口如下

```python
class Op(object):
    def __init__(self):
        pass

    def __call__(self, *inputs):
        return self.forward(*inputs)

    # 前向函数
    # 输入：张量inputs
    # 输出：张量outputs
    def forward(self, *inputs):
        # return outputs
        raise NotImplementedError

    # 反向函数
    # 输入：最终输出对outputs的梯度outputs_grads
    # 输出：最终输出对inputs的梯度inputs_grads
    def backward(self, *outputs_grads):
        # return inputs_grads
        raise NotImplementedError

# --------------------------------------------------------
# 在上面的接口中,forward是自定义Op的前向函数,必须被子类重写,
# 它的参数为输入对象,参数的类型和数量任意;
# backward是自定义Op的反向函数,必须被子类重写,
# 它的参数为forward输出张量的梯度outputs_grads,
# 它的输出为forward输入张量的梯度inputs_grads
# --------------------------------------------------------
```

---
> ***笔记:*** 在torch中, 可以直接调用模型的 forward() 方法进行前向执行, 也可以调用 __call__, 从而执行在 forward() 中定义的前向计算逻辑

---

### 1.1 加法算子

<div align="center">
  <img src="https://camo.githubusercontent.com/8df7a5765a30107bd098831e0b6c9c74b0165b43bf250c3c678a0366d3b0ffc4/68747470733a2f2f61692d73747564696f2d7374617469632d6f6e6c696e652e63646e2e626365626f732e636f6d2f33303862366437383339366234646230613364663534386463623464653937313930306439306264633334613438316239323131323630636432356339333737" alt="加法算子的前反向计算过程" width="800">
</div>

***加法算子的前反向函数:***

1. 前向计算, 加法计算输出 $z = x + y$
2. 反向计算, 假设经过一个其他操作后, 最终输出为 $L$

> 令 $\delta_z=\frac{\partial L}{\partial z}$, $\delta_x=\frac{\partial L}{\partial x}$, $\delta_y=\frac{\partial L}{\partial y}$

> 加法算子的反向计算的输入是梯度 $\delta_z$

> 输出是梯度 $\delta_x$ 和 $\delta_y$

> ***根据链式法则:***

> (利用递归高效计算$y$关于每个函数$f_l$的偏导数的方式是利用递归进行反向计算)$\delta_{l-1} = \frac{\partial f_l}{\partial f_{l-1}} \delta_{l}.$
- $\delta_x =  \delta_z \times 1$
- $\delta_y =  \delta_z \times 1$

```python
class Add(Op):
    def __init__(self):
        super(add, self).__init__()

    def __call__(self, x, y):
        return self.forward(x, y)

    def forward(self, x, y):
        self.x = x
        self.y = y
        outputs = x + y
        return outputs

    def backward(self, grads):
        grads_x = grads * 1
        grads_y = grads * 1
        return grads_x, grads_y

```

> 定义: $x=1$、$y=4$, 根据反向计算, 得到 $x$ 和 $y$ 的梯度

---
***笔记:***
> *args：接收任意多个位置参数（非关键字参数），打包成一个 元组（tuple）

```python
# ------------------------------------------
# 处理"多出来的"位置参数
# ------------------------------------------
def add(*args):
    """
    使用 *args（接受任意多个数字相加）
    """
    print("args =", args)      # args 是一个 tuple
    return sum(args)

print(add(1, 2))              # args = (1, 2) → 3
print(add(1, 2, 3, 4, 5))     # args = (1, 2, 3, 4, 5) → 15
print(add())                  # args = () → 0

# -------------------------------------------

def greet(name, *args):
    """
    也可以和其他参数混合使用(但 *args 必须在后面)
    """
    print(f"Hello {name}!")
    print("Extras:", args)

greet("Alice", "apple", "book", "cat")
# 输出：
# Hello Alice!
# Extras: ('apple', 'book', 'cat')
```
> **kwargs：接收任意多个关键字参数，打包成一个 字典（dict）
```python
# --------------------------------------------
# **kwargs: 处理"多出来的"关键字参数
# --------------------------------------------
def person(name, age, **kwargs):
    """
    使用 **kwargs 接收额外信息
    """
    print(f"{name} is {age} years old")
    print("Other info:", kwargs)     # kwargs 是一个 dict

person("Bob", 25, city="NYC", job="Engineer")
# 输出：
# Bob is 25 years old
# Other info: {'city': 'NYC', 'job': 'Engineer'}
```
---

### 1.2 乘法算子
