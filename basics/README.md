# 张量

> ***深度学习中用向量或矩阵运算提高计算效率*** 
> $\bm{w}^\top\bm{x} = w_1x_1+w_2x_2+\cdots+w_Nx_N$

$\bm{w} = \begin{bmatrix}
w_1w_2 \cdots w_N
\end{bmatrix}^\top$

$\bm{x} = \begin{bmatrix}
x_1x_2 \cdots x_N
\end{bmatrix}^\top$

<!-- $$
\bm{w}^\top = \begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_N
\end{bmatrix}
$$ -->

> 数据常用张量(Tensor)形式存储, 张量是矩阵的扩展与延伸, 认为是高阶的矩阵, 一阶张量是向量, 二阶张量是矩阵, 类似 Numpy 多维数组(ndarray), 可以具有任意多的维度

---
> **注意:** 这里的"维度"是"阶"的概念,和线性代数中向量的"维度"含义不同
---

> 张量大小用形状(shape)描述, 比如一个三维张量的形状是[2,2,5], 表示每一维(也称为轴(axis))的元素的数量, 即第0轴上的数量是2, 第1轴上的数量是2, 第2轴上的数量是5.

<div align="center">
  <img src="https://camo.githubusercontent.com/88c7f6cf2f1eb17d870d33e5cc394dc79a201f0a2e3a8133332efa32ea3aeb91/68747470733a2f2f61692d73747564696f2d7374617469632d6f6e6c696e652e63646e2e626365626f732e636f6d2f66636464353030343731623834326134383131626437616233663732346162346239323236666339346266343436383138393034653539636531626236653030" alt="三种维度的张量可视化表示" width="800">
</div>

> 张量中元素的类型可以是布尔型整数、整数、浮点数或者复数, 但同一张量中所有元素的数据类型均相同, 因此可以给张量一个定义一个数据类型(dtype)来表示其元素的类型

## 1 创建张量

### 1.1 指定数据创建张量

> 通过给定Python列表数据, 可以创建任意维度的张量

```python
ndim_1_tensor = torch.tensor([2.0, 3.0, 4.0])

ndim_2_tensor = torch.tensor([[2.0, 3.0, 4.0],
                                [1.0, 6.0, 7.0]])

ndim_3_tensor = torch.tensor([[[2.0, 3.0, 4.0],
                                [1.0, 6.0, 7.0]],
                                [[2.0, 3.0, 4.0],
                                [1.0, 6.0, 7.0]]])
```

> 需要注意的是,张量在任何一个维度上的元素数量必须相等.下面尝试定义一个在同一维度上元素数量不等的张量

### 1.2 指定形状创建

```python
zeros_tensor = torch.zeros([3, 2])

ones_tensor = torch.ones([3, 2])

full_tensor = torch.full([3, 2], 10)
```
### 1.3 指定区间创建

```python
arange_tensor  = torch.arange(start=1, end=5, step=1)

# res: tensor([1, 2, 3, 4])
```
## 2 张量的属性

### 2.1 张量的形状

> 张量具有如下形状属性

| 方法 | 含义 |
|:---|:---:|
| ***Tensor.ndim*** | 张量的维度 |
| ***Tensor.shape*** | 每个维度上元素的数量 |
| ***Tensor.shape[n]*** | 张量第n维的大小,第n维也称为轴(axis) |
| ***Tensor.size*** | 张量中全部元素的个数 |

<div align="center">
  <img src="https://camo.githubusercontent.com/35616717a23afffd5b23a2a2c7aba20430da47ade37b6b6de3ccd1b909e47642/68747470733a2f2f61692d73747564696f2d7374617469632d6f6e6c696e652e63646e2e626365626f732e636f6d2f64383436316566303939343534396139386331623235336632613331666536306564623762623230306239363462343361396332353831386632326133316336" alt="ndim/shape/axis/size四种属性间的区别" width="700">
</div>

```python
# ---------------------------------
# Input
# ---------------------------------
ones_tensor  = torch.ones([2, 3, 4, 5])
print(ones_tensor.ndim)
print(ones_tensor.shape)
print(ones_tensor.shape[-1])
print(ones_tensor.size())
print(ones_tensor.numel())
print(ones_tensor)

# ---------------------------------
# Output
# ---------------------------------
4
torch.Size([2, 3, 4, 5])
5
torch.Size([2, 3, 4, 5])
120
tensor([[[[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]],

         [[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]],

         [[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]]],


        [[[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]],

         [[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]],

         [[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]]]])
```

### 2.2 改变张量形状

```python
# ------------------------------
# Input
# ------------------------------
ndim_3_Tensor = torch.tensor([[[1, 2, 3, 4, 5],
                              [6, 7, 8, 9, 10]],

                              [[11, 12, 13, 14, 15],
                              [16, 17, 18, 19, 20]],
                              
                              [[21, 22, 23, 24, 25],
                              [26, 27, 28, 29, 30]]])

print(ndim_3_Tensor.shape)
print(ndim_3_Tensor)

reshape_Tensor = torch.reshape(ndim_3_Tensor, [2, 5, 3])
print(reshape_Tensor)

# -------------------------------
# Output
# -------------------------------
torch.Size([3, 2, 5])
tensor([[[ 1,  2,  3,  4,  5],
         [ 6,  7,  8,  9, 10]],

        [[11, 12, 13, 14, 15],
         [16, 17, 18, 19, 20]],

        [[21, 22, 23, 24, 25],
         [26, 27, 28, 29, 30]]])
tensor([[[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9],
         [10, 11, 12],
         [13, 14, 15]],

        [[16, 17, 18],
         [19, 20, 21],
         [22, 23, 24],
         [25, 26, 27],
         [28, 29, 30]]])
```
---
***笔记_reshape技巧:***

- -1 表示这个维度的值是从张量的元素总数和剩余维度推断出来的, 因此, 有且只有一个维度可以设置为 -1

- ~~0 表示实际的维数是从张量的对应维数中复制出来的, 因此shape中0所对应的索引值不能超过张量的总维度~~

---


```python
ndim_1_Tensor = ndim_3_Tensor.reshape([-1])

torch.Size([30])
tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])
```

---

> ***torch.unsqueeze 将张量中的一个或多个维度中插入尺寸为1的维度***

```python
ndim2_Tensor = torch.ones([5, 10])
print(ndim2_Tensor.shape)

new_ndim2_Tensor = torch.unsqueeze(ndim2_Tensor, axis=0)
print(new_ndim2_Tensor)
print(new_ndim2_Tensor.shape)

# -------------------------------------------
# Ouput
# -------------------------------------------
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
torch.Size([5, 10])
tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])
torch.Size([1, 5, 10])
```

### 2.3 张量的数据类型

```python
ndim2_Tensor = torch.ones([5, 10])
print(ndim2_Tensor.dtype)

# --------------------------------------------
# Output
# --------------------------------------------
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
torch.float32
```

> ***.dtype() 查看张量的数据类型***

> ***.to() 改变张量的数据类型***

```python
ndim2_Tensor = torch.ones([5, 10])
new_Tensor= ndim2_Tensor.to(torch.int64)

print(ndim2_Tensor.dtype)
print(new_Tensor.dtype)
print(new_Tensor)
# --------------------------------------------
# Output
# --------------------------------------------
torch.float32
torch.int64
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
```
### 2.4 张量的设备位置

> 初始化张量时可以通过place来指定分配的设备位置, 可支持的设备位置有三种: CPU、GPU和固定内存.

> 固定内存仅针对CPU张量, 用于加速后续CPU到GPU的数据传输

```python
ndim2_Tensor.device
# --------------------------------------------
# Output
# --------------------------------------------
cpu
```

## 3 张量与Numpy数组转换

```python
import torch
ndim2_Tensor = torch.ones([5, 10])

print(ndim2_Tensor.dtype)
print(ndim2_Tensor.device)

print(ndim2_Tensor)

ndim2_Numpy = ndim2_Tensor.numpy()

print(ndim2_Numpy)
print(ndim2_Numpy.dtype)
# --------------------------------------------
# Output
# --------------------------------------------
torch.float32
cpu
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]
float32
```

## 4 张量的访问

### 4.1 索引和切片

> 可以通过索引或切片方便地访问或修改张量

> 使用Python索引规则与Numpy索引规则, 具有以下特点

- 基于 $0 - (n-1)$ 的下标进行索引, 如果下标为负数, 则从尾部开始计算
- 通过冒号":"分隔切片参数start:end:step进行切片操作, 也就是访问start到end范围内的部分元素并生成一个新的序列. 其中start为切片的起始位置, end为切片的截止位置, step是切片的步长, 这三个参数均可缺省.

### 4.2 访问单维张量

> 针对一维张量, 对于单个轴进行索引和切片

```python
ndim_1_Tensor = torch.arange(start=1, end=10, step=1)

print(ndim_1_Tensor)
print(ndim_1_Tensor.dtype)

print(ndim_1_Tensor[0])
print(ndim_1_Tensor[1])
print(ndim_1_Tensor[-1])
print(ndim_1_Tensor[:3])
print(ndim_1_Tensor[::3])

# -------------------------------------------
# Output
# -------------------------------------------
tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
torch.int64
tensor(1)
tensor(2)
tensor(9)
tensor([1, 2, 3])
tensor([1, 4, 7])
```

### 4.3 访问高维张量

```python
ndim_2_Tensor = torch.tensor([[0, 1, 2, 3],
                              [4, 5, 6, 7],
                              [8, 9, 10, 11]])

print(ndim_2_Tensor)

print(ndim_2_Tensor[0])
print(ndim_2_Tensor[0, :])
print(ndim_2_Tensor[:, 0])
print(ndim_2_Tensor[:, -1])
print(ndim_2_Tensor[:])
print(ndim_2_Tensor[0, 1])

# ---------------------------------------------------
# Output
# ---------------------------------------------------
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
tensor([0, 1, 2, 3])
tensor([0, 1, 2, 3])
tensor([0, 4, 8])
tensor([ 3,  7, 11])
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
tensor(1)
```

### 4.4 修改张量

> 与访问张量类似, 可以在单个或多个轴上通过索引或切片操作来修改张量

---
**提醒:**

*慎重通过索引或切片操作来修改张量, 此操作仅会原地修改该张量的数值, 且原值不会被保存. 如果被修改的张量参与梯度计算, 将仅会使用修改后的数值, 这可能会给梯度计算引入风险*

---

### 4.4 