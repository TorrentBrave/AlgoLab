好的，这四种“基于训练的方法”的子类别是理解当前图像编辑技术脉络的关键。以下是它们准确的中文翻译和核心解释：

---

### 1. Training-Based: Reference and Attribute Guided Editing
**基于训练的方法：参考与属性引导编辑**

*   **核心思想**：通过**参考图像**或**特定属性**来精确控制编辑过程。
*   **工作原理**：模型在训练时学习如何根据一个输入条件（如另一张图片、一个属性标签）来修改图像。例如，给模型看一张A的人脸和一张B的人脸，让它把A的表情换成B的；或者直接告诉模型将输入图像的“年龄”属性从“年轻”改为“年老”。
*   **好比**：你给修图师一张目标风格的照片（参考图像）或者一个明确的指令（如“把天空变成黄昏”），他根据这个条件来修改你的照片。

### 2. Training-Based: Domain-Specific Editing
**基于训练的方法：领域特定编辑**

*   **核心思想**：制造一个“**专业修图师**”，他只精通处理某一类特定的图像。
*   **工作原理**：模型在某个狭窄但专业的领域（如人脸、风景画、动漫头像）的大量数据上进行训练，从而成为该领域的专家。它非常擅长修改这类图像，但无法处理它不熟悉的图像类型。
*   **好比**：你有一位专门修人像的顶级摄影师，他修人像是一流水平，但你让他去修一辆汽车或者一幅建筑草图，他可能就无从下手。

### 3. Training-Based: Instructional Editing
**基于训练的方法：指令式编辑**

*   **核心思想**：让模型像理解人类语言一样，通过**自然语言指令**来编辑图像。
*   **工作原理**：模型在大量（图像，指令，编辑后图像）的三元组数据上进行训练。它学习将人类的语言指令（如“把帽子去掉”、“让这条狗穿上雨衣”）映射到具体的像素级修改操作。
*   **好比**：你不需要懂任何专业软件，只需要对助手说“把背景里那个路人P掉”，他就能理解并执行。**InstructPix2Pix** 是这一类的代表作。

### 4. Training-Based: Pseudo-Target Retrieval-Based Editing
**基于训练的方法：基于伪目标检索的编辑**

*   **核心思想**：当没有明确的“正确答案”（编辑后的图像）时，通过**检索**和**评分**来“创造”一个训练目标。
*   **工作原理**：这是一种“走捷径”的训练策略。当想根据文本编辑图像但没有真实的编辑后图片作为监督时，模型会：
    1.  **检索**：从一个庞大的图库中，找到一些与编辑文本描述相似的图像，作为“伪目标”。
    2.  **评分**：使用一个评分模型（如CLIP）来判断编辑结果与目标文本的匹配程度。
    3.  **学习**：模型利用这个检索到的“伪目标”或评分信号作为监督信号来训练自己。
*   **好比**：老师不知道你作文的确切范文，但他从一大堆优秀作文里找了几篇风格接近的，告诉你：“就往这个方向改，改得越像这些文章越好。”

---

### 总结对比

| 中文翻译 | 核心输入 | 编辑方式 | 好比 |
| :--- | :--- | :--- | :--- |
| **参考与属性引导编辑** | 参考图、属性标签 | 精确的条件控制 | 给样图或具体参数 |
| **领域特定编辑** | 特定领域图像 | 专业领域内的修改 | 找专项专家 |
| **指令式编辑** | 自然语言指令 | 理解并执行指令 | 用语言指挥助手 |
| **基于伪目标检索的编辑**| 文本描述 | 通过检索和模仿学习 | 参考范文进行模仿 |

希望这些解释能帮助您清晰地理解这四类方法的区别与联系。

您提出了一个非常关键且敏锐的问题！您的理解基本正确，但这里存在一个核心的差异，我通过一个表格和比喻来厘清：

### 核心区别：如何定义“修改目标”

所有这些方法都接受一张**输入图片**，但**如何确定要把它修改成什么样子**，是它们之间最根本的区别。

| 方法类别 | 输入 | “修改目标”由什么定义？ | 好比 |
| :--- | :--- | :--- | :--- |
| **指令式编辑** | 图片 + **自然语言指令** | **指令本身** (e.g., “把帽子去掉”) | **指挥官**：你告诉助手“把左边那个人P掉”，他理解并执行。 |
| **参考与属性引导编辑** | 图片 + **参考图/属性** | **另一张图片或一个标签** (e.g., 另一张人脸、”微笑”属性) | **临摹**：你给修图师一张范冰冰的照片说：“把我的脸P成她的发型和妆容。” |
| **领域特定编辑** | 图片 + **(可选)简单信号** | **模型在训练中学到的领域知识** (e.g., 所有人脸的结构) | **专家直觉**：你把人像照片给专家，说“修好看点”。他基于经验调整肤色、液化等，不需要你细说。 |
| **基于伪目标检索的编辑** | 图片 + **文本描述** | **从图库中检索出的、最匹配文本的图片** | **找范文**：你想写“春天”，但不知怎么写。你去文库搜“春天”，找到一篇范文，然后模仿着写。 |

---

### 详细解释与类比

让我们用同一个场景来对比这四种方法。假设我们有一张**一个人在公园里的照片**。

1.  **指令式编辑**
    - **输入**：照片 + 指令 `“把背景换成东京夜景”`
    - **过程**：模型直接理解“背景”、“换成”、“东京夜景”这些概念，并执行像素级操作。
    - **特点**：**最灵活、最人性化**。用户可以用丰富的语言描述任何想象到的改变。

2.  **参考与属性引导编辑**
    - **输入**：照片 + **一张东京夜景的图片**（作为参考图）
    - **过程**：模型学习将参考图的“风格”或“内容”（东京夜景）迁移到输入图片的背景上。
    - **特点**：**更精确，但需要素材**。当用户难以用语言描述“东京夜景”时，直接给一张图更准确。

3.  **领域特定编辑**
    - **输入**：主要就是这张**照片本身**。它之所以能被编辑，是因为它属于模型训练过的“自然图像”或“室外场景”领域。
    - **过程**：如果你想美化这张照片，你调用的可能是一个“风景美化”专家模型，它基于学到的“好看风景”的知识来优化颜色、对比度等。
    - **特点**：**专业化强，但功能固定**。你很难让一个“人像美颜”的领域模型去完成“换背景”这种任务。

4.  **基于伪目标检索的编辑**
    - **输入**：照片 + 文本描述 `“一个人在东京夜景中”`
    - **过程**：模型内部不直接理解“东京夜景”，而是会去一个庞大的图库里，找到那些最符合“一个人在东京夜景中”这个描述的图片，然后学着让自己的输出向那些检索到的图片靠拢。
    - **特点**：一种**“曲线救国”的训练策略**，常用于早期或数据不足时，让模型学会文本和图像的关联。

### 结论

您说的没错，**所有图像编辑任务的核心都是“修改一张输入图片”**。

它们的根本区别在于 **“指导这次修改的控制信号”** 是什么：
- **指令式编辑**：信号是**人类的自然语言**。
- **参考引导编辑**：信号是**另一张视觉素材**。
- **属性引导编辑**：信号是**一个分类标签**（如“年轻”、“微笑”）。
- **领域特定编辑**：信号是**模型内化的专业知识**，用户输入可能很简单。
- **伪目标检索编辑**：信号是**文本描述，但通过检索机制间接实现**。

所以，指令式编辑是其中**交互最自然、门槛最低**的一种方式，因为它直接使用了我们最习惯的沟通工具——语言。


# 论文方法分类

## 基于训练的方法
- **基于弱监督的领域特定编辑**
- **通过自监督的参考与属性引导**
- **通过全监督的指令编辑**
- **基于弱监督的伪目标检索**

## 测试时微调方法
- **去噪模型微调**
- **嵌入微调**
- **使用超网络的指导**
- **潜在变量优化**
- **混合微调**

## 无需训练与微调的方法
- **输入文本优化**
- **反转/采样修改**
- **注意力修改**
- **掩码指导**
- **多噪声重定向**

# 基准评测 EditEval_v1

# LMM Score 计算模板

# 排行榜

# 星标历史


# Diffusion Model_Based image Editing

## Abstract
> Denoising diffusion models have emerged **(已崛起为)** as a powerful tool for various image generation and editing tasks, facilitating **(促进)** the synthesis of visual content in an unconditional or input-conditional manner.

```markdown
# 基于扩散模型的图像编辑：综述

Yi Huang1, Jiancheng Huang1, Yifan Liu1, Mingfu Yan1, Jiaxi Lv1, Jianzhuang Liu1, , Wei Xiong, He Zhang, Liangliang Cao, , and Shifeng Chen

**摘要**  
去噪扩散模型已成为各种图像生成和编辑任务的有力工具，能够以无条件或输入条件的方式合成视觉内容。其核心思想是学习逆转逐渐向图像添加噪声的过程，从而能够从复杂分布中生成高质量样本。本综述详尽概述了利用扩散模型进行图像编辑的现有方法，涵盖该领域的理论和实践方面。我们从学习策略、用户输入条件以及可完成的具体编辑任务等多个角度对这些工作进行了深入分析和分类。此外，我们特别关注图像修复和外绘，探索了早期的传统上下文驱动方法和当前的多模态条件方法，并对其方法论进行了全面分析。为了进一步评估文本引导图像编辑算法的性能，我们提出了一个系统性基准 **EditEval**，并引入了一个创新性指标 **LMM Score**。最后，我们讨论了当前局限性并展望了未来研究的一些潜在方向。附带的代码库已发布在 [https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods](https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods)。

**关键词**：扩散模型，图像编辑，AIGC

## 1 引言

在AIGC领域，图像生成和编辑因其在数字媒体、广告和科学研究等实际应用中的重要性而受到认可。与从最小输入创建全新图像的图像生成不同，图像编辑涉及修改现有图像的外观、结构或内容以达到期望结果。这个过程可以涵盖从细微调整到适度变换的一系列更改，而不会从根本上改变原始图像。图像编辑的演进已从手动、劳动密集型过程发展到先进的基于学习的算法。生成对抗网络（GANs）的引入是这一演进中的关键进展。

最近，扩散模型已成为该领域的有力工具。扩散模型通过逐渐向数据添加噪声，然后学习从随机噪声中逆转此过程，直到生成与源数据分布匹配的期望数据。扩散模型在图像编辑中的应用已引起越来越多的兴趣，这突显了扩散模型在提高图像编辑性能方面相较于先前方法的潜力和多功能性。

鉴于这一显著进展，系统性地回顾和总结这些贡献至关重要。然而，尽管取得了这些进展，目前仍缺乏专门针对基于扩散模型的图像编辑的综述。现有的关于扩散模型的综述倾向于关注其他视觉任务，而那些提及图像编辑的通常只提供粗略概述，缺乏对方法的详细和聚焦探索。为填补这一空白，我们进行了本综述，专门针对使用扩散模型进行2D图像编辑提供深入全面的分析。

具体来说，我们深入探讨了该领域中扩散模型实现的方法论、输入条件以及广泛的编辑任务。本综述批判性地回顾了100多篇研究论文，根据其学习策略是否需要训练、在推理期间进行微调，或者可以在没有两者的情况下有效运行，将它们组织成三个主要类别——**基于训练的方法**、**测试时微调方法**和**无需训练与微调的方法**。每个类别根据其核心技术进一步划分。我们还探索了这些方法中使用的10种不同类型的输入条件，包括文本、掩码、参考图像、类别、布局、姿态、草图、分割图、音频和拖拽点，以展示扩散模型在不同图像编辑场景中的适应性。此外，我们的综述提出了一种新的图像编辑任务分类法，将其分为三大类：**语义编辑**、**风格编辑**和**结构编辑**，涵盖了12种具体类型。

我们还特别关注修复和外绘，它们共同构成了一种独特的编辑类型。我们探索了早期的传统方法和当前的多模态条件方法。我们还介绍了 **EditEval**，一个旨在评估文本引导图像编辑算法的基准。特别是，通过利用大型多模态模型（LMMs）的高级视觉-语言理解能力，提出了一个有效的评估指标 **LMM Score**。最后，我们提出了一些当前的挑战和潜在的未来趋势作为展望。

总之，本综述旨在系统地对基于扩散模型的图像编辑的广泛研究进行分类和批判性评估。我们的目标是提供一个全面的资源，不仅综合当前发现，而且指导这个快速发展的领域未来的研究方向。

## 2 背景

### 2.1 扩散模型

扩散模型对生成式AI领域产生了深远影响，涌现了大量相关方法。本质上，这些模型基于一个称为扩散的关键原理，该原理逐渐将某个分布的数据样本添加噪声，将其转换为预定义的通常简单的分布（如高斯分布），然后迭代地逆转此过程以生成与原始分布匹配的数据。扩散模型与早期生成模型的不同之处在于它们在迭代时间步上的动态执行，涵盖了时间上的前向和后向移动。

#### 前向扩散过程
该过程将数据分布转换为预定义分布。变换表示为：
\[ q(\mathbf{z}_t \mid \mathbf{z}_{t-1}) = \mathcal{N}(\mathbf{z}_t \mid \sqrt{1-\beta_t} \mathbf{z}_{t-1}, \beta_t\mathbf{I}) \]
可以简化为单步方程：
\[ q(\mathbf{z}_t \mid \mathbf{z}_0) = \mathcal{N}(\mathbf{z}_t \mid \sqrt{\bar{\alpha}_t} \mathbf{z}_0, (1-\bar{\alpha}_t)\mathbf{I}) \]
其中 \(\alpha_t = 1 - \beta_t\)，\(\bar{\alpha}_t = \prod_{i=1}^{t} \alpha_i\)。因此，\(\mathbf{z}_t\) 可以直接通过以下方式采样：
\[ \mathbf{z}_t = \sqrt{\bar{\alpha}_t} \cdot \mathbf{z}_0 + \sqrt{1-\bar{\alpha}_t} \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \]

#### 反向扩散过程
反向过程，也称为祖先采样，定义为采样随机高斯噪声 \(\mathbf{z}_T\)，然后对 \(t = T, T-1, ..., 1\) 采样 \(\mathbf{z}_{t-1}\)，直到得到 \(\mathbf{z}_0\)。主要目标是学习前向扩散过程的逆过程，旨在生成与原始未更改数据样本 \(\mathbf{z}_0\) 紧密对齐的分布。在图像编辑中，\(\mathbf{z}_0\) 代表编辑后的图像。实践中，这通常使用 U-Net 架构来学习 \(p\) 的参数化版本。

#### 优化
指导反向扩散学习的优化策略涉及最小化前向和反向序列联合分布之间的 KL 散度。Ho 等人采用了一个加权的更简单的去噪损失：
\[ \mathbb{E}_{t\sim\mathcal{U}(1,T),\mathbf{z}_0\sim q(\mathbf{z}_0),\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I})} \left[\lambda(t)\|\epsilon - \epsilon_\theta(\mathbf{z}_t, t)\|^2 \right] \]

#### DDIM 采样与反转
当处理真实图像 \(\mathbf{z}_0\) 时，流行的编辑方法首先使用特定的反转方案将此 \(\mathbf{z}_0\) 反转为相应的 \(\mathbf{z}_T\)。随后，采样从这个 \(\mathbf{z}_T\) 开始，采用某种编辑策略来产生编辑结果 \(\tilde{\mathbf{z}}_0\)。理想情况下，直接从 \(\mathbf{z}_T\) 采样而不进行任何编辑应该产生一个与 \(\mathbf{z}_0\) 非常相似的 \(\tilde{\mathbf{z}}_0\)。\(\tilde{\mathbf{z}}_0\) 与 \(\mathbf{z}_0\) 的显著偏差（称为重建失败）表明编辑后的图像无法保持 \(\mathbf{z}_0\) 中未更改区域的完整性。

#### 文本条件与无分类器指导
文本条件扩散模型设计用于在文本提示 \(P\) 的引导下从随机噪声 \(\mathbf{z}_T\) 合成结果。在采样过程的推理过程中，使用噪声估计网络 \(\epsilon_\theta(\mathbf{z}_t, t, C)\) 来预测噪声 \(\epsilon\)，其中 \(C = \psi(P)\) 表示文本嵌入。

为了确保文本对生成输出的实质性影响和控制，Ho 等人引入了无分类器指导的概念，该技术结合了条件预测和无条件预测。具体来说，令 \(\varnothing = \psi("")\) 表示空文本嵌入。与指导尺度 \(w\) 结合时，无分类器指导预测形式化为：
\[ \epsilon_\theta(\mathbf{z}_t, t, C, \varnothing) = w\epsilon_\theta(\mathbf{z}_t, t, C) + (1-w) \epsilon_\theta(\mathbf{z}_t, t, \varnothing) \]
在此公式中，\(\epsilon_\theta(\mathbf{z}_t, t, C, \varnothing)\) 取代了采样方程中的 \(\epsilon_\theta(\mathbf{z}_t, t)\)。\(w\) 的值通常在 \([1, 7.5]\) 范围内，决定了文本控制的程度。较高的 \(w\) 值与生成过程中更强的文本驱动影响相关。

### 2.2 Stable Diffusion 中的注意力机制

#### Stable Diffusion 中的交叉注意力
在 Stable Diffusion 中，交叉注意力层在融合图像和文本方面起着至关重要的作用，允许 T2I 模型生成与文本描述一致的图像。交叉注意力层接收来自噪声图像和提示的查询、键和值矩阵，即 \(Q_{cross}\)、\(K_{cross}\) 和 \(V_{cross}\)。交叉注意力图定义为：
\[ M_{cross} = \text{Softmax}\left(\frac{Q_{cross}K_{cross}^{\top}}{\sqrt{d_{cross}}}\right) \]
最终输出定义为文本和图像的融合特征，表示为 \(M_{cross}V_{cross}\)。

#### Stable Diffusion 中的自注意力
与交叉注意力不同，自注意力层通过线性层从噪声图像接收键矩阵 \(K_{self}\) 和查询矩阵 \(Q_{self}\)。自注意力图定义为：
\[ M_{self} = \text{Softmax}\left(\frac{Q_{self}K_{self}^{\top}}{\sqrt{d_{self}}}\right) \]
\(M_{self}\) 中的 \(M_{self}^{ij}\) 决定了分配给图像中第 \(i\) 个和第 \(j\) 个空间特征相关性的权重，并可以影响生成图像的空间布局和形状细节。

### 2.3 相关任务

#### 2.3.1 条件图像生成
*   **类条件图像生成**：早期努力通常通过在采样期间通过额外的预训练分类器合并类诱导的梯度。Ho 等人引入了不依赖外部分类器的无分类器指导，并允许更通用的条件（例如文本）作为指导。
*   **文本到图像（T2I）生成**：GLIDE 是第一个直接使用文本从高维像素级别指导图像生成的工作。Imagen 使用级联框架在像素空间中更有效地生成高分辨率图像。另一研究方向首先将图像投影到低维空间，然后在此潜在空间中应用扩散模型。代表工作包括 Stable Diffusion (SD)、VQ-diffusion 和 DALL-E 2。
*   **附加条件**：除了文本之外，还使用更具体的条件来实现图像合成中更高的保真度和更精确的控制。
*   **个性化图像生成**：与条件图像生成中的图像编辑密切相关的是创建个性化图像的任务。该任务侧重于生成保持某种身份的图像，通常由同一主体的少量参考图像引导。两种早期方法是 Textual Inversion 和 DreamBooth。
*   **矢量图形生成**：Scalable Vector Graphics (SVGs) 可以缩放到任何尺寸，并且对于数字图标、图形和贴纸来说很紧凑。VectorFusion 表明，在图像的像素表示上训练的文本条件扩散模型可用于生成可导出为 SVG 的矢量图形。

#### 2.3.2 图像恢复与增强
*   **输入图像作为条件**：生成模型已为各种图像恢复任务（如超分辨率和去模糊）做出了显著贡献。
*   **非空间空间中的恢复**：一些基于扩散模型的 IR 方法专注于其他空间。
*   **T2I 先验使用**：合并 T2I 信息被证明是有利的，因为它允许使用预训练的 T2I 模型。
*   **基于投影的方法**：这些方法旨在从输入图像中提取固有结构或纹理，以在每一步补充生成的图像，并确保数据一致性。
*   **基于分解的方法**：这些方法将 IR 任务视为线性逆问题。

#### 2.3.3 图像合成
*   **基于图像的虚拟试穿**：基于图像的虚拟试穿（VTON）的目标是生成一个人穿着所选服装的图像，同时保留该人的身份并确保视觉连贯性。
*   **图像引导的对象合成**：图像引导的对象合成从用户提供的照片中合并特定对象和场景，可能借助文本提示。

## 3 范围与分类

**范围**：本综述专注于那些最直接贡献于使用扩散模型进行图像编辑领域的工作。为确保清晰和聚焦的讨论，本综述应用两个主要标准来纳入工作：（1）任务必须专注于图像编辑，定义为修改现有图像的外观、结构或内容，而不进行构成新图像生成的重大转换；（2）方法必须依赖于在 2D 图像上执行的扩散模型。

**分类**：在本综述中，我们根据学习策略将基于扩散模型的图像编辑论文组织成三个主要组：**基于训练的方法**、**测试时微调方法**和**无需训练与微调的方法**。这些学习策略根据方法是否需要大量训练阶段、在推理阶段涉及微调，或者可以在没有两者的情况下执行来定义。此外，我们探索了这些方法使用的 10 种输入条件，以及 12 种最常见的编辑类型，这些类型被组织成三个广泛的类别，定义如下：

*   **语义编辑**：此类包括对图像内容和叙事的更改，影响所描绘场景的故事、上下文或主题元素。此类中的任务包括对象添加、对象移除、对象替换、背景更改和情感表达修改。
*   **风格编辑**：此类侧重于增强或转换图像的视觉风格和美学元素，而不改变其叙事内容。此类中的任务包括颜色更改、纹理更改和整体风格更改（包括艺术和写实风格）。
*   **结构编辑**：此类涉及图像内元素的空间排列、定位、视点和特征的更改，强调场景内对象的组织和呈现。此类中的任务包括对象移动、对象大小和形状更改、对象动作和姿态更改以及透视/视点更改。

## 4 基于训练的方法

在基于扩散模型的图像编辑领域，基于训练的方法指的是在应用于特定编辑任务之前需要在大型数据集上进行大量训练阶段的方法。这些方法以其稳定的训练过程和对数据分布的有效建模而著称，从而在各种编辑场景中产生可靠的性能。

### 4.1 领域特定编辑

早期的研究在较小的专用数据集上训练这些模型，这些数据集高度集中于特定领域，例如用于人脸操作的 CelebA 和 FFHQ，用于动物面部编辑和翻译的 AFHQ，用于对象修改的 LSUN，以及用于风格迁移的 WikiArt。

*   **CLIP 指导**：受基于 GAN 的方法使用 CLIP 进行文本引导图像编辑的启发，一些研究将 CLIP 纳入扩散模型。一个关键例子是 DiffusionCLIP。
*   **循环正则化**：由于扩散模型能够进行领域转换，CycleGAN 等方法中使用的循环框架也在其中进行了探索。
*   **投影与插值**：GANs 中经常使用的另一种技术涉及将两个真实图像投影到 GAN 潜在空间，然后在它们之间进行插值以实现平滑的图像操作，这也被一些扩散模型采用用于图像编辑。
*   **分类器指导**：一些研究通过引入额外的预训练分类器进行指导来增强图像编辑性能。

### 4.2 参考与属性引导编辑

这类工作从单个图像中提取属性或其他信息，以自监督的方式作为训练基于扩散的图像编辑模型的条件。它们可以分为两种类型：基于参考的图像合成和属性控制的图像编辑。

*   **基于参考的图像合成**：为了学习如何合成图像，PbE 以自监督的方式进行训练。
*   **属性控制的图像编辑**：这类论文通常涉及使用特定的图像特征作为控制条件来增强预训练的扩散模型，以学习生成相应的图像。

### 4.3 指令编辑

使用指令（例如，“移除帽子”）来驱动图像编辑过程，而不是使用编辑后图像的描述（例如，“一只戴帽子微笑的小狗”），似乎更自然、更人性化，也更符合用户需求。

*   **InstructPix2Pix 框架**：使扩散模型能够根据指令编辑图像的一个主要挑战是构建指令-图像配对数据集。InstructPix2Pix 分两步生成这些图像对。
*   **模型架构增强**：MoEController 引入了混合专家（MOE）架构。FoI 利用 InstructPix2Pix 的隐式接地能力来识别和聚焦于特定的编辑区域。
*   **数据质量增强**：LOFIE 通过利用分割、思维链提示和视觉问答（VQA）的最新进展来增强训练数据集的质量。
*   **人类反馈增强学习**：为了改善编辑后图像与人类指令之间的一致性，HIVE 在指令图像编辑中引入了来自人类反馈的强化学习（RLHF）。
*   **视觉指令**：提出了 ImageBrush 来从一对说明所需操作的变换图像中学习视觉指令，并将此指令应用于编辑新图像。
*   **利用多模态大规模模型**：InstructAny2Pix 允许用户通过整合音频、图像和文本的指令来编辑图像。

### 4.4 基于伪目标检索的编辑

由于获得准确代表真实情况的编辑图像具有挑战性，此类方法旨在检索伪目标图像或直接使用 CLIP 分数作为优化模型参数的目标。

## 5 测试时微调方法

测试时微调方法通常不涉及传统的预训练阶段，而是在推理（测试时）阶段需要对每个待编辑的特定图像进行微调。这些方法旨在优化模型对单个图像的性能，从而实现更精确和个性化的编辑。

### 5.1 去噪模型微调

去噪模型是图像生成和编辑的核心组件。直接对其进行微调是一种简单有效的方法。

*   **微调整个去噪模型**：微调整个去噪模型允许模型更好地学习图像的特定特征并更准确地解释文本提示，从而产生更符合用户意图的编辑。
*   **去噪模型中的部分参数微调**：一些方法专注于微调去噪模型的特定部分，例如自注意力层、交叉注意力层、编码器或解码器。

### 5.2 嵌入微调

许多微调方法选择针对文本或空文本嵌入进行优化，以更好地将嵌入与生成过程集成，从而实现增强的编辑结果。

*   **空文本嵌入微调**：空文本嵌入微调的目标是解决 DDIM 反转中的重建失败问题，从而提高与原始图像的一致性。
*   **文本嵌入微调**：微调从输入文本导出的嵌入可以增强图像编辑，使编辑后的图像更符合条件特征。

### 5.3 使用超网络的指导

除了传统的生成框架，一些方法还集成了一个自定义网络，以更好地与特定的编辑意图对齐。

### 5.4 潜在变量优化

图像潜在变量的直接优化也是在微调过程中采用的一种技术。这种方法涉及通过引入某些损失函数关系和某些中间层的特征来直接优化噪声潜在变量，而不是优化生成器的参数或嵌入的条件参数。

*   **人为引导的潜在变量优化**：这种方法允许用户参与图像编辑过程，指导图像的生成。
*   **利用网络层和输入优化潜在变量**：一些优化方法利用从输入条件导出的嵌入或网络特征来构建损失函数，从而实现潜在变量的直接优化。

### 5.5 混合微调

一些工作结合了上述各种微调方法，这些方法可以是顺序的，具有连续发生的调优阶段，或者作为单个集成工作流的一部分同时进行。

*   **文本嵌入和去噪模型微调**：Imagic 分阶段实现其目标。
*   **文本编码器和去噪模型微调**：SINE 最初对文本编码器和去噪模型进行微调，以更好地理解单个图像的内容和几何结构。

## 6 无需训练与微调的方法

无需训练和微调的方法从快速和低成本的前提开始。它们在编辑过程中不需要任何形式的训练（在数据集上）或微调（在源图像上）。

### 6.1 输入文本优化

输入文本优化在图像编辑领域标志着改进文本到图像翻译机制的重要进展。这种方法侧重于文本嵌入的增强和用户输入的简化，试图保证图像根据给定文本准确且上下文相关地进行修改。

*   **输入文本嵌入的增强**：这些方法侧重于文本嵌入的优化以修改文本到图像的翻译。
*   **基于指令的文本指导**：此类使用户能够通过用户指令对图像编辑过程进行细粒度控制。

### 6.2 反转/采样修改

反转和采样公式的修改是无需训练和微调方法中的常见技术。反转过程用于将真实图像反转为噪声潜在空间，然后采样过程用于在给定目标提示的情况下生成编辑结果。

*   **重建信息记忆**：DDPM Inversion、SDE-Drag、LEDITS++、FEC 和 EMILIE 都属于重建信息记忆类别；它们在反转阶段保存信息并在采样阶段使用它以确保重建。
*   **在采样中利用空文本**：Negative Inversion、ProxEdit 和 Null-Text Guidance 都探索了空文本（也称为负向提示）信息对图像编辑的影响。
*   **单步多噪声预测**：EDICT 和 AIDI 提出在单步采样期间预测多个噪声以解决重建失败问题。

### 6.3 注意力修改

注意力修改方法增强了注意力层中的操作，这是最常见的直接无需训练的图像编辑方式。在 Stable Diffusion 的 U-Net 中，有许多交叉注意力和自注意力层，这些层中的注意力图和特征图包含大量语义信息。注意力修改的共同特点是识别注意力层中的内在原理，然后通过修改注意力操作来利用它们。

*   **注意力图替换**：P2P 引入了一个直观的提示到提示编辑框架，仅依赖于文本输入，通过识别交叉注意力层作为控制图像布局和提示词之间空间关系的关键。
*   **注意力特征替换**：MasaCtrl 和 PnP 都强调替换注意力特征以保持一致性。
*   **局部注意力图修改**：TF-ICON 框架和 Object-Shape Variation 共享局部注意力图修改的利用。
*   **注意力分数指导**：Conditional Score Guidance 和 EBMs 都致力于使用不同的注意力分数指导。

### 6.4 掩码指导

基于扩散的图像编辑中的掩码指导代表了一种用于增强图像编辑的技术。它包括通过选择性处理提高去噪效率的技术，用于目标图像编辑的精确掩码自动生成，以及掩码引导的区域聚焦以确保与特定感兴趣区域准确对齐的局部修改。

*   **掩码增强的去噪效率**：这些方法利用掩码来增强基于扩散的图像编辑的效率。
*   **掩码自动生成**：这些方法通过自动生成掩码来实现精确的图像编辑。
*   **掩码引导的区域聚焦**：这类研究使用掩码作为导航工具，引导采样过程朝向特定的感兴趣区域。

### 6.5 多噪声重定向

多噪声重定向是在单步中预测不同方向的多个噪声，然后将它们重定向为单个噪声的过程。这种重定向的优点在于它能够使单个噪声统一多个不同的编辑方向，从而更有效地满足用户的编辑需求。

*   **语义噪声引导**：这些方法在语义上引导采样过程中的噪声重定向，并增强对图像内容的细粒度控制。
*   **对象感知的噪声重定向**：OIR-Diffusion 引入了对象感知的反转和重组方法，通过确定每个样本的最佳反转步骤并无缝集成编辑区域与其他区域，从而实现对图像中特定对象的细粒度编辑。

## 7 修复与外绘

图像修复和外绘通常被视为图像编辑的子任务，具有独特的目标和挑战。我们将它们分为两种主要类型以便更好地解释。

### 7.1 传统上下文驱动修复

#### 7.1.1 监督训练
基于监督训练的修复和外绘方法通常使用配对的损坏和完整图像从头开始训练扩散模型。

#### 7.1.2 零样本学习
*   **上下文先验集成**：这类方法从未损坏的图像部分提取结构和纹理以补充掩码区域，确保全局内容一致性。
*   **退化分解**：图像修复可以被视为一般线性逆问题的专门应用。
*   **后验估计**：为了解决一般噪声线性逆问题，一些研究侧重于估计后验分布 \(p(\mathbf{x}|\mathbf{y})\)，利用基于贝叶斯定理的无条件扩散模型。

### 7.2 多模态条件修复

#### 7.2.1 随机掩码训练
传统的图像修复，侧重于由周围上下文驱动的填充图像缺失区域，通常缺乏对内容的精确控制。随着文本到图像扩散模型的进步，通过注入额外的多模态条件（如文本描述、分割图和参考图像）正在克服这一限制。

#### 7.2.2 精确控制条件
随机掩码的一个基本问题是它可能覆盖与文本提示无关的区域，导致模型忽略提示，尤其是当掩码区域较小时。为了提供更精确的控制，SmartBrush 引入了一个精确度因子来产生各种掩码类型。

#### 7.2.3 预训练扩散先验利用
尽管基于训练的方法取得了显著进展，但仍然存在挑战，特别是在收集大规模真实数据和模型训练所需的大量资源方面。因此，一些研究将重点转向特定的修复任务，通过集成各种技术来利用预训练文本到图像扩散模型的生成能力。

### 7.3 外绘

图像外绘，类似于但不完全相同于修复，旨在生成无缝扩展图像边界的新像素。现有的 T2I 模型（如 Stable Diffusion 和 DALL-E）由于其在不同大小和形状的图像的大型数据集上的训练，可以推广到解决此任务。它通常被视为具有类似实现的修复的一种特殊形式。

## 8 基准与评估

### 8.1 基准构建

在之前的部分中，我们深入研究了基于扩散模型的图像编辑方法的方法论方面。除了此分析之外，评估这些方法，检查它们在各种编辑任务中的能力至关重要。然而，现有的图像编辑基准有限，并不能完全满足我们综述中确定的需求。

为了解决这些问题，我们引入了 **EditEval**，一个旨在评估通用基于扩散模型的图像编辑方法的基准。EditEval 包括一个精心策划的 150 张高质量图像的数据集，每张图像都配有文本提示。EditEval 评估从表 I 中选择的 7 个常见编辑任务的性能。此外，我们提出了 **LMM Score**，一个利用大型多模态模型（LMMs）的能力来评估不同任务编辑性能的定量评估指标。除了 LMM Score 提供的客观评估之外，我们还进行了用户研究以纳入主观评估。

#### 8.1.1 任务选择
在选择评估任务时，我们考虑了表 I 中指示的能力。观察到大多数方法能够处理语义和风格任务，同时在结构编辑方面遇到挑战。潜在原因是许多当前 T2I 扩散模型（大多数编辑方法依赖于此）在准确的空间感知方面存在困难。考虑到这些因素和实际应用，我们为我们的基准选择了七个常见任务：对象添加、对象替换、对象移除、背景替换、风格更改、纹理更改和动作更改，旨在从简单对象编辑到复杂场景更改全面评估编辑方法的性能。

#### 8.1.2 数据集构建
对于图像选择，我们手动从 Unsplash 的专业照片在线存储库中选择了 150 张图像，确保主题和场景的广泛多样性。这些图像被裁剪为正方形格式，与大多数编辑模型的输入比例对齐。然后，我们将选定的图像分类为 7 组，每组与上述特定编辑任务之一相关联。

在为每张图像生成提示时，我们使用 LMM 来创建描述图像内容的源提示、概述编辑预期结果的目标提示以及相应的指令来指导编辑过程。此步骤通过向 GPT-4V 提供详细模板来促进。在 GPT-4V 生成初始提示和指令后，我们进行了细致的检查，以确保每个提示和指令集对于相应的图像和任务是具体、清晰且直接适用的。

最终数据集，包括选定的图像、它们的源和目标提示以及编辑指令，可在附带的 GitHub 存储库中找到。

#### 8.1.3 指标设计与选择
图像编辑领域传统上依赖 CLIPScore 作为主要的定量评估指标。尽管它在评估图像和相应文本提示之间的一致性方面有效，但 CLIPScore 在具有许多细节和特定空间关系的复杂场景中可能会遇到困难。这种局限性促使需要一种更通用和全面的指标，可应用于更广泛的图像编辑任务。因此，我们提出了 **LMM Score**，一种通过利用大型多模态模型（LMMs）的高级视觉-语言理解能力的新指标。

为了开发此指标，我们首先指导 GPT-4 构想一个定量指标，并概述一个框架，允许通过适当的用户提示对通用图像编辑任务进行客观评估。基于 GPT-4 的建议，评估框架包含以下要素。标准整合了四个关键因素：
*   **编辑准确性**：评估编辑后的图像 \(I^{i}_{tgt}\) 与指定的编辑提示 \(t_{edit}\) 和指令 \(t_{inst}\) 的贴合程度，衡量编辑的精确度。
*   **上下文保留**：评估编辑后的图像 \(I^{i}_{tgt}\) 在不需要更改的方面保留源图像 \(I_{src}\) 上下文的能力。
*   **视觉质量**：评估编辑后图像 \(I^{i}_{tgt}\) 的整体质量，包括分辨率、无伪影、颜色准确性、清晰度等因素。
*   **逻辑真实感**：评估编辑后图像 \(I^{i}_{tgt}\) 在逻辑上的真实感，包括对自然法则的遵守，如光照一致性、纹理连续性等。

至于评估，我们使用 LMM（本工作中具体为 GPT-4V）来执行此评估。GPT-4V 遵循用户预定义的指令计算每个编辑图像在四个评估因素上的子分数，获得如公式 15 所示的 \(S_{LMM}\) 值。

除了 LMM Score 提供的客观评估之外，我们还进行了用户研究以收集主观反馈。这项研究邀请了来自不同背景的 50 名参与者，确保了广泛的观点。每位参与者会看到源图像、其描述、编辑提示和指令以及一系列编辑后的图像。然后要求他们根据 LMM Score 使用的相同四个评估因素对每个编辑后的图像进行评分。然后汇总参与者的分数，以使用公式 15 中的相同计算计算每个图像的整体用户评分（\(S_{User}\)）。此用户研究的结果补充了 LMM Score 评估，提供了对不同编辑方法性能的全面评估。

### 8.2 评估

#### 8.2.1 方法选择
对于每个编辑任务的评估，我们从表 I 中仔细选择了 4 到 8 种方法，涵盖了一系列方法，包括基于训练的、测试时微调的和无需训练与微调的方法。为确保公平和一致的比较，我们的选择标准如下：方法必须仅需要文本条件，拥有处理特定任务的能力，并且拥有可用于实现的开源代码。我们排除了领域特定的方法，以避免其在不同领域中应用的局限性所带来的约束。

#### 8.2.2 比较分析
*   **性能比较**：为了对所选方法在 7 个选定编辑任务上进行全面评估，我们计算了所有评估样本上 \(S_{LMM}\) 和 \(S_{User}\) 的均值和标准差。
*   **LMM Score 与用户研究之间的相关性**：为了检验 LMM Score 的有效性，我们调查了它与用户研究之间的相关性。
*   **LMM Score 与其他指标的对比**：为了进一步评估 LMM Score 的有效性，我们通过检查它们与用户研究结果的皮尔逊相关系数，将其与另外三个指标——CLIPScore、Directional CLIP Similarity 和 TIFA Score——进行了比较。

## 9 挑战与未来方向

尽管在使用扩散模型进行图像编辑方面取得了成功，但在未来的工作中仍有一些局限性需要解决。

*   **更少步数的模型推理**：大多数基于扩散的模型在推理期间需要大量步骤来获得最终图像，这既耗时又计算成本高，给模型部署和用户体验带来了挑战。
*   **高效模型**：训练一个能够生成真实结果的扩散模型计算密集，并且需要大量高质量数据。
*   **复杂对象结构编辑**：现有作品在编辑图像时可以合成真实的颜色、风格或纹理。然而，在处理复杂结构时，它们仍然会产生明显的伪影。
*   **复杂光照和阴影编辑**：编辑图像的光照或照明仍然是一项具有挑战性的任务，尤其是在追求逼真和一致的结果时。
*   **图像编辑的不鲁棒性**：现有的基于扩散的图像编辑模型可以为一部分给定条件合成逼真的视觉内容。然而，它们在许多真实世界场景中仍然失败。
*   **高分辨率图像生成与编辑**：高分辨率图像（通常指分辨率为 1024x1024 像素或更高）在各种应用中变得越来越重要。
*   **可靠的评估指标**：准确的评估对于图像编辑至关重要，以确保编辑内容与给定条件良好对齐。

## 10 结论

我们已经广泛概述了基于扩散模型的图像编辑方法，从多个角度审视了该领域。我们的分析首先根据学习策略将 100 多种方法分为三个主要组：基于训练的、测试时微调和无需训练与微调的方法。然后，我们将图像编辑任务分为三个不同的类别：语义、风格和结构编辑，总共涵盖了 12 种具体类型。我们探讨了这些方法及其在提高编辑性能方面的贡献。在我们的图像编辑基准 EditEval 内，对 7 个任务以及最新的最先进方法进行了评估。此外，引入了一个新的指标 LMM Score 用于这些方法的比较分析。在结束我们的综述时，我们强调了图像编辑领域的广泛潜力，并提出了未来研究的方向。

---
**注意**：此 README.md 是对原始综述论文的翻译和总结。详细信息、完整引用和图表请参阅原始论文和附带资源。
```